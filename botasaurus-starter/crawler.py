import os
import json
import re
from datetime import datetime
from time import sleep
from src.scrape_heading_task import start_crawl_ggmap


def run_crawler_logic(inputs):
    """
    H√†m n√†y nh·∫≠n inputs t·ª´ giao di·ªán v√† th·ª±c hi·ªán crawl
    """
    output_dir = os.path.join("output", "data")
    os.makedirs(output_dir, exist_ok=True)

    logs = []  # ƒê·ªÉ tr·∫£ v·ªÅ giao di·ªán hi·ªÉn th·ªã
    results_paths = []

    for item in inputs:
        msg_start = f"üöÄ ƒêang x·ª≠ l√Ω: {item['type']} - {item['ward']}, {item['county']}, {item['city']}"
        print(msg_start)
        logs.append(msg_start)

        # T·∫°o query
        query = f"{item['type']} {item['street']} {item['ward']} {item['county']} {item['city']} {item['province']} Vi·ªát Nam"

        # --- G·ªçi h√†m crawl ---
        try:
            data = start_crawl_ggmap(item)  # G·ªçi h√†m th·ª±c t·∫ø c·ªßa b·∫°n

            # --- L∆∞u file ---
            safe_name = re.sub(r'[^\w\s-]', '', query.lower())
            safe_name = re.sub(r'[\s]+', '_', safe_name).strip('-_')
            if len(safe_name) > 100:
                safe_name = safe_name[:100]

            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            file_name = f"{timestamp}_{safe_name}.json"
            file_path = os.path.join(output_dir, file_name)

            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)

            msg_success = f"‚úÖ ƒê√£ l∆∞u: {file_name}"
            logs.append(msg_success)
            results_paths.append(file_path)

        except Exception as e:
            msg_err = f"‚ùå L·ªói: {e}"
            logs.append(msg_err)

    return logs, results_paths
